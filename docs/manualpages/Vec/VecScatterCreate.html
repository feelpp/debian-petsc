<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML3.2 EN">
<HTML>
<HEAD> <link rel="canonical" href="http://www.mcs.anl.gov/petsc/petsc-current/docs/manualpages/Vec/VecScatterCreate.html" />
<META NAME="GENERATOR" CONTENT="DOCTEXT">
<TITLE>VecScatterCreate</TITLE>
</HEAD>
<BODY BGCOLOR="FFFFFF">
   <div id="version" align=right><b>petsc-3.4.2 2013-07-02</b></div>
<A NAME="VecScatterCreate"><H1>VecScatterCreate</H1></A>
Creates a vector scatter context. 
<H3><FONT COLOR="#CC3333">Synopsis</FONT></H3>
<PRE>
#include "petscvec.h"    
PetscErrorCode  VecScatterCreate(Vec xin,IS ix,Vec yin,IS iy,VecScatter *newctx)
</PRE>
Collective on <A HREF="../Vec/Vec.html#Vec">Vec</A>
<P>
<H3><FONT COLOR="#CC3333">Input Parameters</FONT></H3>
<TABLE border="0" cellpadding="0" cellspacing="0">
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>xin </B></TD><TD>- a vector that defines the shape (parallel data layout of the vector)
of vectors from which we scatter
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>yin </B></TD><TD>- a vector that defines the shape (parallel data layout of the vector)
of vectors to which we scatter
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>ix </B></TD><TD>- the indices of xin to scatter (if NULL scatters all values)
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>iy </B></TD><TD>- the indices of yin to hold results (if NULL fills entire vector yin)
</TD></TR></TABLE>
<P>
<H3><FONT COLOR="#CC3333">Output Parameter</FONT></H3>
<DT><B>newctx </B> -location to store the new scatter context
<br>
<P>
Options Database Keys: (uses regular MPI_Sends by default)
<TABLE border="0" cellpadding="0" cellspacing="0">
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-vecscatter_view         </B></TD><TD>- Prints detail of communications
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-vecscatter_view ::ascii_info    </B></TD><TD>- Print less details about communication
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-vecscatter_ssend        </B></TD><TD>- Uses MPI_Ssend_init() instead of MPI_Send_init()
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-vecscatter_rsend           </B></TD><TD>- use ready receiver mode for MPI sends
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-vecscatter_merge        </B></TD><TD>- <A HREF="../Vec/VecScatterBegin.html#VecScatterBegin">VecScatterBegin</A>() handles all of the communication, <A HREF="../Vec/VecScatterEnd.html#VecScatterEnd">VecScatterEnd</A>() is a nop
eliminates the chance for overlap of computation and communication
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-vecscatter_sendfirst    </B></TD><TD>- Posts sends before receives
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-vecscatter_packtogether </B></TD><TD>- Pack all messages before sending, receive all messages before unpacking
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-vecscatter_alltoall     </B></TD><TD>- Uses MPI all to all communication for scatter
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-vecscatter_window       </B></TD><TD>- Use MPI 2 window operations to move data
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-vecscatter_nopack       </B></TD><TD>- Avoid packing to work vector when possible (if used with -vecscatter_alltoall then will use MPI_Alltoallw()
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-vecscatter_reproduce    </B></TD><TD>- insure that the order of the communications are done the same for each scatter, this under certain circumstances
will make the results of scatters deterministic when otherwise they are not (it may be slower also).
</TD></TR></TABLE>
<P>
<pre>
</pre>
<pre>
                                                                                   --When packing is used--
</pre>
<pre>
                              MPI Datatypes (no packing)  sendfirst   merge        packtogether  persistent*
</pre>
<pre>
                               _nopack                   _sendfirst    _merge      _packtogether                -vecscatter_
</pre>
<pre>
----------------------------------------------------------------------------------------------------------------------------
</pre>
<pre>
   Message passing    Send       p                           X            X           X         always
</pre>
<pre>
                     Ssend       p                           X            X           X         always          _ssend
</pre>
<pre>
                     Rsend       p                        nonsense        X           X         always          _rsend
</pre>
<pre>
   AlltoAll  v or w              X                        nonsense     always         X         nonsense        _alltoall
</pre>
<pre>
   MPI_Win                       p                        nonsense        p           p         nonsense        _window
</pre>
<pre>
</pre>
<pre>
  Since persistent sends and receives require a constant memory address they can only be used when data is packed into the work vector
</pre>
<pre>
  because the in and out array may be different for each call to <A HREF="../Vec/VecScatterBegin.html#VecScatterBegin">VecScatterBegin</A>/End().
</pre>
<pre>
</pre>
<pre>
   p indicates possible, but not implemented. X indicates implemented
</pre>
<pre>
</pre>
<P>

<P>
<H3><FONT COLOR="#CC3333">Notes</FONT></H3>
In calls to <A HREF="../Vec/VecScatter.html#VecScatter">VecScatter</A>() you can use different vectors than the xin and
yin you used above; BUT they must have the same parallel data layout, for example,
they could be obtained from <A HREF="../Vec/VecDuplicate.html#VecDuplicate">VecDuplicate</A>().
A <A HREF="../Vec/VecScatter.html#VecScatter">VecScatter</A> context CANNOT be used in two or more simultaneous scatters;
that is you cannot call a second <A HREF="../Vec/VecScatterBegin.html#VecScatterBegin">VecScatterBegin</A>() with the same scatter
context until the <A HREF="../Vec/VecScatterEnd.html#VecScatterEnd">VecScatterEnd</A>() has been called on the first <A HREF="../Vec/VecScatterBegin.html#VecScatterBegin">VecScatterBegin</A>().
In this case a separate <A HREF="../Vec/VecScatter.html#VecScatter">VecScatter</A> is needed for each concurrent scatter.
<P>
Currently the MPI_Send(), MPI_Ssend() and MPI_Rsend() all use PERSISTENT versions.
(this unfortunately requires that the same in and out arrays be used for each use, this
is why when not using MPI_alltoallw() we always need to pack the input into the work array before sending
and unpack upon receeving instead of using MPI datatypes to avoid the packing/unpacking).
<P>
Both ix and iy cannot be NULL at the same time.
<P>
<P>
<H3><FONT COLOR="#CC3333">See Also</FONT></H3>
 <A HREF="../Vec/VecScatterDestroy.html#VecScatterDestroy">VecScatterDestroy</A>(), <A HREF="../Vec/VecScatterCreateToAll.html#VecScatterCreateToAll">VecScatterCreateToAll</A>(), <A HREF="../Vec/VecScatterCreateToZero.html#VecScatterCreateToZero">VecScatterCreateToZero</A>()
<BR><P><B><P><B><FONT COLOR="#CC3333">Level:</FONT></B>intermediate
<BR><FONT COLOR="#CC3333">Location:</FONT></B><A HREF="../../../src/vec/vec/utils/vscat.c.html#VecScatterCreate">src/vec/vec/utils/vscat.c</A>
<BR><A HREF="./index.html">Index of all Vec routines</A>
<BR><A HREF="../../index.html">Table of Contents for all manual pages</A>
<BR><A HREF="../singleindex.html">Index of all manual pages</A>
<P><H3><FONT COLOR="#CC3333">Examples</FONT></H3>
<A HREF="../../../src/dm/examples/tutorials/ex6.c.html">src/dm/examples/tutorials/ex6.c.html</A><BR>
<A HREF="../../../src/dm/examples/tutorials/ex6f90.F.html">src/dm/examples/tutorials/ex6f90.F.html</A><BR>
<A HREF="../../../src/ksp/ksp/examples/tutorials/ex49.c.html">src/ksp/ksp/examples/tutorials/ex49.c.html</A><BR>
</BODY></HTML>
