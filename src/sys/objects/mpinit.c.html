<center><a href="mpinit.c">Actual source code: mpinit.c</a></center><br>

<html>
<head> <link rel="canonical" href="http://www.mcs.anl.gov/petsc/petsc-current/src/sys/objects/mpinit.c.html" />
<title></title>
<meta name="generator" content="c2html 0.9.5">
<meta name="date" content="2013-07-02T14:39:03+00:00">
</head>

<body bgcolor="#FFFFFF">
   <div id="version" align=right><b>petsc-3.4.2 2013-07-02</b></div>
<pre width="80">

<a name="line3">  3: </a><font color="#A020F0">#include &lt;petscsys.h&gt;        </font><font color="#B22222">/*I  "petscsys.h"   I*/</font><font color="#A020F0"></font>

<a name="line5">  5: </a>static <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> saved_PETSC_COMM_WORLD = 0;
<a name="line6">  6: </a><A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>        PETSC_COMM_LOCAL_WORLD = 0;           <font color="#B22222">/* comm for a single node (local set of processes) */</font>
<a name="line7">  7: </a><A href="../../../docs/manualpages/Sys/PetscBool.html#PetscBool">PetscBool</A>       PetscHMPIWorker        = <A href="../../../docs/manualpages/Sys/PETSC_FALSE.html#PETSC_FALSE">PETSC_FALSE</A>; <font color="#B22222">/* this is a regular process, nonworker process */</font>
<a name="line8">  8: </a>void            * PetscHMPICtx         = 0;

<a name="line10"> 10: </a><strong><font color="#4169E1">extern <A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscHMPIHandle.html#PetscHMPIHandle">PetscHMPIHandle</A>(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>)</font></strong>;

<a name="line12"> 12: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPI_COMM_SPAWN)</font>
<a name="line15"> 15: </a><font color="#B22222">/*@C</font>
<a name="line16"> 16: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscHMPISpawn.html#PetscHMPISpawn">PetscHMPISpawn</A> - Initialize additional processes to be used as "worker" processes. This is not generally</font>
<a name="line17"> 17: </a><font color="#B22222">     called by users. One should use -hmpi_spawn_size &lt;n&gt; to indicate that you wish to have n-1 new MPI</font>
<a name="line18"> 18: </a><font color="#B22222">     processes spawned for each current process.</font>

<a name="line20"> 20: </a><font color="#B22222">   Not Collective (could make collective on MPI_COMM_WORLD, generate one huge comm and then split it up)</font>

<a name="line22"> 22: </a><font color="#B22222">   Input Parameter:</font>
<a name="line23"> 23: </a><font color="#B22222">.  nodesize - size of each compute node that will share processors</font>

<a name="line25"> 25: </a><font color="#B22222">   Options Database:</font>
<a name="line26"> 26: </a><font color="#B22222">.   -hmpi_spawn_size nodesize</font>

<a name="line28"> 28: </a><font color="#B22222">   Notes: This is only supported on systems with an MPI 2 implementation that includes the MPI_Comm_Spawn() routine.</font>

<a name="line30"> 30: </a><font color="#B22222">$    Comparison of two approaches for HMPI usage (MPI started with N processes)</font>
<a name="line31"> 31: </a><font color="#B22222">$</font>
<a name="line32"> 32: </a><font color="#B22222">$    -hmpi_spawn_size &lt;n&gt; requires MPI 2, results in n*N total processes with N directly used by application code</font>
<a name="line33"> 33: </a><font color="#B22222">$                                           and n-1 worker processes (used by PETSc) for each application node.</font>
<a name="line34"> 34: </a><font color="#B22222">$                           You MUST launch MPI so that only ONE MPI process is created for each hardware node.</font>
<a name="line35"> 35: </a><font color="#B22222">$</font>
<a name="line36"> 36: </a><font color="#B22222">$    -hmpi_merge_size &lt;n&gt; results in N total processes, N/n used by the application code and the rest worker processes</font>
<a name="line37"> 37: </a><font color="#B22222">$                            (used by PETSc)</font>
<a name="line38"> 38: </a><font color="#B22222">$                           You MUST launch MPI so that n MPI processes are created for each hardware node.</font>
<a name="line39"> 39: </a><font color="#B22222">$</font>
<a name="line40"> 40: </a><font color="#B22222">$    petscmpiexec -n 2 ./ex1 -hmpi_spawn_size 3 gives 2 application nodes (and 4 PETSc worker nodes)</font>
<a name="line41"> 41: </a><font color="#B22222">$    petscmpiexec -n 6 ./ex1 -hmpi_merge_size 3 gives the SAME 2 application nodes and 4 PETSc worker nodes</font>
<a name="line42"> 42: </a><font color="#B22222">$       This is what would use if each of the computers hardware nodes had 3 CPUs.</font>
<a name="line43"> 43: </a><font color="#B22222">$</font>
<a name="line44"> 44: </a><font color="#B22222">$      These are intended to be used in conjunction with USER HMPI code. The user will have 1 process per</font>
<a name="line45"> 45: </a><font color="#B22222">$   computer (hardware) node (where the computer node has p cpus), the user's code will use threads to fully</font>
<a name="line46"> 46: </a><font color="#B22222">$   utilize all the CPUs on the node. The PETSc code will have p processes to fully use the compute node for</font>
<a name="line47"> 47: </a><font color="#B22222">$   PETSc calculations. The user THREADS and PETSc PROCESSES will NEVER run at the same time so the p CPUs</font>
<a name="line48"> 48: </a><font color="#B22222">$   are always working on p task, never more than p.</font>
<a name="line49"> 49: </a><font color="#B22222">$</font>
<a name="line50"> 50: </a><font color="#B22222">$    See <A href="../../../docs/manualpages/PC/PCHMPI.html#PCHMPI">PCHMPI</A> for a PETSc preconditioner that can use this functionality</font>
<a name="line51"> 51: </a><font color="#B22222">$</font>

<a name="line53"> 53: </a><font color="#B22222">   For both <A href="../../../docs/manualpages/Sys/PetscHMPISpawn.html#PetscHMPISpawn">PetscHMPISpawn</A>() and <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A>() <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> consists of one process per "node", PETSC_COMM_LOCAL_WORLD</font>
<a name="line54"> 54: </a><font color="#B22222">   consists of all the processes in a "node."</font>

<a name="line56"> 56: </a><font color="#B22222">   In both cases the user's code is running ONLY on <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> (that was newly generated by running this command).</font>

<a name="line58"> 58: </a><font color="#B22222">   Level: developer</font>

<a name="line60"> 60: </a><font color="#B22222">   Concepts: HMPI</font>

<a name="line62"> 62: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscFinalize.html#PetscFinalize">PetscFinalize</A>(), PetscInitializeFortran(), <A href="../../../docs/manualpages/Sys/PetscGetArgs.html#PetscGetArgs">PetscGetArgs</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIFinalize.html#PetscHMPIFinalize">PetscHMPIFinalize</A>(), <A href="../../../docs/manualpages/Sys/PetscInitialize.html#PetscInitialize">PetscInitialize</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIRun.html#PetscHMPIRun">PetscHMPIRun</A>()</font>

<a name="line64"> 64: </a><font color="#B22222">@*/</font>
<a name="line65"> 65: </a><strong><font color="#4169E1"><a name="PetscHMPISpawn"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscHMPISpawn.html#PetscHMPISpawn">PetscHMPISpawn</A>(<A href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</A> nodesize)</font></strong>
<a name="line66"> 66: </a>{

<a name="line68"> 68: </a>  <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,PETSC_ERR_SUP,<font color="#666666">"HMPI functionality is currently broken"</font>);
<a name="line69"> 69: </a><font color="#A020F0">#if defined(broken_functionality_commented_out)</font>
<a name="line71"> 71: </a>  <A href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</A>    size;
<a name="line72"> 72: </a>  <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>       parent,children;

<a name="line75"> 75: </a>  MPI_Comm_get_parent(&amp;parent);
<a name="line76"> 76: </a>  <font color="#4169E1">if</font> (parent == MPI_COMM_NULL) {  <font color="#B22222">/* the original processes started by user */</font>
<a name="line77"> 77: </a>    char programname[PETSC_MAX_PATH_LEN];
<a name="line78"> 78: </a>    char **argv;

<a name="line80"> 80: </a>    <A href="../../../docs/manualpages/Sys/PetscGetProgramName.html#PetscGetProgramName">PetscGetProgramName</A>(programname,PETSC_MAX_PATH_LEN);
<a name="line81"> 81: </a>    <A href="../../../docs/manualpages/Sys/PetscGetArguments.html#PetscGetArguments">PetscGetArguments</A>(&amp;argv);
<a name="line82"> 82: </a>    MPI_Comm_spawn(programname,argv,nodesize-1,MPI_INFO_NULL,0,<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,&amp;children,MPI_ERRCODES_IGNORE);
<a name="line83"> 83: </a>    <A href="../../../docs/manualpages/Sys/PetscFreeArguments.html#PetscFreeArguments">PetscFreeArguments</A>(argv);
<a name="line84"> 84: </a>    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Intercomm_merge.html#MPI_Intercomm_merge">MPI_Intercomm_merge</A>(children,0,&amp;PETSC_COMM_LOCAL_WORLD);

<a name="line86"> 86: </a>    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A>,&amp;size);
<a name="line87"> 87: </a>    PetscInfo2(0,<font color="#666666">"PETSc HMPI successfully spawned: number of nodes = %d node size = %d\n"</font>,size,nodesize);

<a name="line89"> 89: </a>    saved_PETSC_COMM_WORLD = <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A>;
<a name="line90"> 90: </a>  } <font color="#4169E1">else</font> { <font color="#B22222">/* worker nodes that get spawned */</font>
<a name="line91"> 91: </a>    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Intercomm_merge.html#MPI_Intercomm_merge">MPI_Intercomm_merge</A>(parent,1,&amp;PETSC_COMM_LOCAL_WORLD);
<a name="line92"> 92: </a>    <A href="../../../docs/manualpages/Sys/PetscHMPIHandle.html#PetscHMPIHandle">PetscHMPIHandle</A>(PETSC_COMM_LOCAL_WORLD);
<a name="line93"> 93: </a>    PetscHMPIWorker = <A href="../../../docs/manualpages/Sys/PETSC_TRUE.html#PETSC_TRUE">PETSC_TRUE</A>; <font color="#B22222">/* so that <A href="../../../docs/manualpages/Sys/PetscHMPIFinalize.html#PetscHMPIFinalize">PetscHMPIFinalize</A>() will not attempt a broadcast from this process */</font>
<a name="line94"> 94: </a>    <A href="../../../docs/manualpages/Sys/PetscEnd.html#PetscEnd">PetscEnd</A>();  <font color="#B22222">/* cannot continue into user code */</font>
<a name="line95"> 95: </a>  }
<a name="line96"> 96: </a>  <font color="#4169E1">return</font>(0);
<a name="line97"> 97: </a><font color="#A020F0">#endif</font>
<a name="line98"> 98: </a>}
<a name="line99"> 99: </a><font color="#A020F0">#endif</font>

<a name="line103">103: </a><font color="#B22222">/*@C</font>
<a name="line104">104: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A> - Initializes the PETSc and MPI to work with HMPI. This is not usually called</font>
<a name="line105">105: </a><font color="#B22222">      by the user. One should use -hmpi_merge_size &lt;n&gt; to indicate the node size of merged communicator</font>
<a name="line106">106: </a><font color="#B22222">      to be.</font>

<a name="line108">108: </a><font color="#B22222">   Collective on MPI_COMM_WORLD or <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> if it has been set</font>

<a name="line110">110: </a><font color="#B22222">   Input Parameter:</font>
<a name="line111">111: </a><font color="#B22222">+  nodesize - size of each compute node that will share processors</font>
<a name="line112">112: </a><font color="#B22222">.  func - optional function to call on the master nodes</font>
<a name="line113">113: </a><font color="#B22222">-  ctx - context passed to function on master nodes</font>

<a name="line115">115: </a><font color="#B22222">   Options Database:</font>
<a name="line116">116: </a><font color="#B22222">.   -hmpi_merge_size &lt;n&gt;</font>

<a name="line118">118: </a><font color="#B22222">   Level: developer</font>

<a name="line120">120: </a><font color="#B22222">$    Comparison of two approaches for HMPI usage (MPI started with N processes)</font>
<a name="line121">121: </a><font color="#B22222">$</font>
<a name="line122">122: </a><font color="#B22222">$    -hmpi_spawn_size &lt;n&gt; requires MPI 2, results in n*N total processes with N directly used by application code</font>
<a name="line123">123: </a><font color="#B22222">$                                           and n-1 worker processes (used by PETSc) for each application node.</font>
<a name="line124">124: </a><font color="#B22222">$                           You MUST launch MPI so that only ONE MPI process is created for each hardware node.</font>
<a name="line125">125: </a><font color="#B22222">$</font>
<a name="line126">126: </a><font color="#B22222">$    -hmpi_merge_size &lt;n&gt; results in N total processes, N/n used by the application code and the rest worker processes</font>
<a name="line127">127: </a><font color="#B22222">$                            (used by PETSc)</font>
<a name="line128">128: </a><font color="#B22222">$                           You MUST launch MPI so that n MPI processes are created for each hardware node.</font>
<a name="line129">129: </a><font color="#B22222">$</font>
<a name="line130">130: </a><font color="#B22222">$    petscmpiexec -n 2 ./ex1 -hmpi_spawn_size 3 gives 2 application nodes (and 4 PETSc worker nodes)</font>
<a name="line131">131: </a><font color="#B22222">$    petscmpiexec -n 6 ./ex1 -hmpi_merge_size 3 gives the SAME 2 application nodes and 4 PETSc worker nodes</font>
<a name="line132">132: </a><font color="#B22222">$       This is what would use if each of the computers hardware nodes had 3 CPUs.</font>
<a name="line133">133: </a><font color="#B22222">$</font>
<a name="line134">134: </a><font color="#B22222">$      These are intended to be used in conjunction with USER HMPI code. The user will have 1 process per</font>
<a name="line135">135: </a><font color="#B22222">$   computer (hardware) node (where the computer node has p cpus), the user's code will use threads to fully</font>
<a name="line136">136: </a><font color="#B22222">$   utilize all the CPUs on the node. The PETSc code will have p processes to fully use the compute node for</font>
<a name="line137">137: </a><font color="#B22222">$   PETSc calculations. The user THREADS and PETSc PROCESSES will NEVER run at the same time so the p CPUs</font>
<a name="line138">138: </a><font color="#B22222">$   are always working on p task, never more than p.</font>
<a name="line139">139: </a><font color="#B22222">$</font>
<a name="line140">140: </a><font color="#B22222">$    See <A href="../../../docs/manualpages/PC/PCHMPI.html#PCHMPI">PCHMPI</A> for a PETSc preconditioner that can use this functionality</font>
<a name="line141">141: </a><font color="#B22222">$</font>

<a name="line143">143: </a><font color="#B22222">   For both <A href="../../../docs/manualpages/Sys/PetscHMPISpawn.html#PetscHMPISpawn">PetscHMPISpawn</A>() and <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A>() <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> consists of one process per "node", PETSC_COMM_LOCAL_WORLD</font>
<a name="line144">144: </a><font color="#B22222">   consists of all the processes in a "node."</font>

<a name="line146">146: </a><font color="#B22222">   In both cases the user's code is running ONLY on <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> (that was newly generated by running this command).</font>

<a name="line148">148: </a><font color="#B22222">   Concepts: HMPI</font>

<a name="line150">150: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscFinalize.html#PetscFinalize">PetscFinalize</A>(), PetscInitializeFortran(), <A href="../../../docs/manualpages/Sys/PetscGetArgs.html#PetscGetArgs">PetscGetArgs</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIFinalize.html#PetscHMPIFinalize">PetscHMPIFinalize</A>(), <A href="../../../docs/manualpages/Sys/PetscInitialize.html#PetscInitialize">PetscInitialize</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPISpawn.html#PetscHMPISpawn">PetscHMPISpawn</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIRun.html#PetscHMPIRun">PetscHMPIRun</A>()</font>

<a name="line152">152: </a><font color="#B22222">@*/</font>
<a name="line153">153: </a><strong><font color="#4169E1"><a name="PetscHMPIMerge"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A>(<A href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</A> nodesize,<A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A> (*func)(void*),void *ctx)</font></strong>
<a name="line154">154: </a>{

<a name="line156">156: </a>  <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,PETSC_ERR_SUP,<font color="#666666">"HMPI functionality is currently broken"</font>);
<a name="line157">157: </a><font color="#A020F0">#if defined(broken_functionality_commented_out)</font>
<a name="line159">159: </a>  <A href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</A>    size,rank,*ranks,i;
<a name="line160">160: </a>  MPI_Group      group,newgroup;

<a name="line163">163: </a>  saved_PETSC_COMM_WORLD = <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A>;

<a name="line165">165: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</A>(saved_PETSC_COMM_WORLD,&amp;size);
<a name="line166">166: </a>  <font color="#4169E1">if</font> (size % nodesize) <A href="../../../docs/manualpages/Sys/SETERRQ2.html#SETERRQ2">SETERRQ2</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,PETSC_ERR_ARG_SIZ,<font color="#666666">"Total number of process nodes %d is not divisible by number of processes per node %d"</font>,size,nodesize);
<a name="line167">167: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_rank.html#MPI_Comm_rank">MPI_Comm_rank</A>(saved_PETSC_COMM_WORLD,&amp;rank);


<a name="line170">170: </a>  <font color="#B22222">/* create two communicators</font>
<a name="line171">171: </a><font color="#B22222">      *) one that contains the first process from each node: 0,nodesize,2*nodesize,...</font>
<a name="line172">172: </a><font color="#B22222">      *) one that contains all processes in a node:  (0,1,2...,nodesize-1), (nodesize,nodesize+1,...2*nodesize-), ...</font>
<a name="line173">173: </a><font color="#B22222">  */</font>
<a name="line174">174: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_group.html#MPI_Comm_group">MPI_Comm_group</A>(saved_PETSC_COMM_WORLD,&amp;group);
<a name="line175">175: </a>  <A href="../../../docs/manualpages/Sys/PetscMalloc.html#PetscMalloc">PetscMalloc</A>((size/nodesize)*<font color="#4169E1">sizeof</font>(<A href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</A>),&amp;ranks);
<a name="line176">176: </a>  <font color="#4169E1">for</font> (i=0; i&lt;(size/nodesize); i++) ranks[i] = i*nodesize;
<a name="line177">177: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Group_incl.html#MPI_Group_incl">MPI_Group_incl</A>(group,size/nodesize,ranks,&amp;newgroup);
<a name="line178">178: </a>  <A href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</A>(ranks);
<a name="line179">179: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_create.html#MPI_Comm_create">MPI_Comm_create</A>(saved_PETSC_COMM_WORLD,newgroup,&amp;<A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A>);
<a name="line180">180: </a>  <font color="#4169E1">if</font> (rank % nodesize) <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> = 0; <font color="#B22222">/* mark invalid processes for easy debugging */</font>
<a name="line181">181: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Group_free.html#MPI_Group_free">MPI_Group_free</A>(&amp;group);
<a name="line182">182: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Group_free.html#MPI_Group_free">MPI_Group_free</A>(&amp;newgroup);

<a name="line184">184: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_split.html#MPI_Comm_split">MPI_Comm_split</A>(saved_PETSC_COMM_WORLD,rank/nodesize,rank % nodesize,&amp;PETSC_COMM_LOCAL_WORLD);

<a name="line186">186: </a>  PetscInfo2(0,<font color="#666666">"PETSc HMPI successfully started: number of nodes = %d node size = %d\n"</font>,size/nodesize,nodesize);
<a name="line187">187: </a>  PetscInfo1(0,<font color="#666666">"PETSc HMPI process %sactive\n"</font>,(rank % nodesize) ? <font color="#666666">"in"</font> : <font color="#666666">""</font>);

<a name="line189">189: </a>  PetscHMPICtx = ctx;
<a name="line190">190: </a>  <font color="#B22222">/*</font>
<a name="line191">191: </a><font color="#B22222">     All process not involved in user application code wait here</font>
<a name="line192">192: </a><font color="#B22222">  */</font>
<a name="line193">193: </a>  <font color="#4169E1">if</font> (!<A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A>) {
<a name="line194">194: </a>    <A href="../../../docs/manualpages/Sys/PetscHMPIHandle.html#PetscHMPIHandle">PetscHMPIHandle</A>(PETSC_COMM_LOCAL_WORLD);
<a name="line195">195: </a>    <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> = saved_PETSC_COMM_WORLD;
<a name="line196">196: </a>    PetscHMPIWorker  = <A href="../../../docs/manualpages/Sys/PETSC_TRUE.html#PETSC_TRUE">PETSC_TRUE</A>; <font color="#B22222">/* so that <A href="../../../docs/manualpages/Sys/PetscHMPIFinalize.html#PetscHMPIFinalize">PetscHMPIFinalize</A>() will not attempt a broadcast from this process */</font>
<a name="line197">197: </a>    <A href="../../../docs/manualpages/Profiling/PetscInfo.html#PetscInfo">PetscInfo</A>(0,<font color="#666666">"PETSc HMPI inactive process becoming active"</font>);
<a name="line198">198: </a>  } <font color="#4169E1">else</font> <font color="#4169E1">if</font> (func) {
<a name="line199">199: </a>    (*func)(ctx);
<a name="line200">200: </a>  }
<a name="line201">201: </a>  <font color="#4169E1">return</font>(0);
<a name="line202">202: </a><font color="#A020F0">#endif</font>
<a name="line203">203: </a>}

<a name="line207">207: </a><font color="#B22222">/*@C</font>
<a name="line208">208: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscHMPIFinalize.html#PetscHMPIFinalize">PetscHMPIFinalize</A> - Finalizes the PETSc and MPI to work with HMPI. Called by <A href="../../../docs/manualpages/Sys/PetscFinalize.html#PetscFinalize">PetscFinalize</A>() cannot</font>
<a name="line209">209: </a><font color="#B22222">       be called by user.</font>

<a name="line211">211: </a><font color="#B22222">   Collective on the entire system</font>

<a name="line213">213: </a><font color="#B22222">   Level: developer</font>

<a name="line215">215: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscFinalize.html#PetscFinalize">PetscFinalize</A>(), <A href="../../../docs/manualpages/Sys/PetscGetArgs.html#PetscGetArgs">PetscGetArgs</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A>(), PCHMPIRun()</font>

<a name="line217">217: </a><font color="#B22222">@*/</font>
<a name="line218">218: </a><strong><font color="#4169E1"><a name="PetscHMPIFinalize"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscHMPIFinalize.html#PetscHMPIFinalize">PetscHMPIFinalize</A>(void)</font></strong>
<a name="line219">219: </a>{
<a name="line220">220: </a>  <A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A> 0;
<a name="line221">221: </a>  <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       command = 3;

<a name="line224">224: </a>  <font color="#4169E1">if</font> (!PetscHMPIWorker &amp;&amp; PETSC_COMM_LOCAL_WORLD) {
<a name="line225">225: </a>    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;command,1,MPIU_INT,0,PETSC_COMM_LOCAL_WORLD); <font color="#B22222">/* broadcast to my worker group to end program */</font>

<a name="line227">227: </a>    <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> = saved_PETSC_COMM_WORLD;

<a name="line229">229: </a>    <A href="../../../docs/manualpages/Profiling/PetscInfo.html#PetscInfo">PetscInfo</A>(0,<font color="#666666">"PETSc HMPI active process ending <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A>()"</font>);
<a name="line230">230: </a>  }
<a name="line231">231: </a>  <A href="../../../docs/manualpages/Sys/PetscFunctionReturn.html#PetscFunctionReturn">PetscFunctionReturn</A>(ierr);
<a name="line232">232: </a>}

<a name="line234">234: </a>static <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A> numberobjects = 0;
<a name="line235">235: </a>static void     *objects[100];

<a name="line239">239: </a><font color="#B22222">/*@C</font>
<a name="line240">240: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscHMPIHandle.html#PetscHMPIHandle">PetscHMPIHandle</A> - Receives commands from the master node and processes them</font>

<a name="line242">242: </a><font color="#B22222">   Collective on <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A></font>

<a name="line244">244: </a><font color="#B22222">   Input Parameter:</font>
<a name="line245">245: </a><font color="#B22222">.   comm - Must be PETSC_COMM_LOCAL_WORLD</font>

<a name="line247">247: </a><font color="#B22222">   Level: developer</font>

<a name="line249">249: </a><font color="#B22222">   Notes: this is usually handled automatically, likely you do not need to use this directly</font>

<a name="line251">251: </a><font color="#B22222">   Developer Notes: Since comm must be PETSC_COMM_LOCAL_WORLD, why have this argument?</font>

<a name="line253">253: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A>(), PCHMPIRun(), PCHMPINew()</font>

<a name="line255">255: </a><font color="#B22222">@*/</font>
<a name="line256">256: </a><strong><font color="#4169E1"><a name="PetscHMPIHandle"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscHMPIHandle.html#PetscHMPIHandle">PetscHMPIHandle</A>(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> comm)</font></strong>
<a name="line257">257: </a>{
<a name="line259">259: </a>  <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       command       = 0; <font color="#B22222">/* dummy value so MPI-Uni doesn't think it is not set*/</font>
<a name="line260">260: </a>  <A href="../../../docs/manualpages/Sys/PetscBool.html#PetscBool">PetscBool</A>      exitwhileloop = <A href="../../../docs/manualpages/Sys/PETSC_FALSE.html#PETSC_FALSE">PETSC_FALSE</A>;

<a name="line263">263: </a>  <font color="#4169E1">while</font> (!exitwhileloop) {
<a name="line264">264: </a>    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;command,1,MPIU_INT,0,comm);
<a name="line265">265: </a>    <font color="#4169E1">switch</font> (command) {
<a name="line266">266: </a>    <font color="#4169E1">case</font> 0: { <font color="#B22222">/* allocate some memory on this worker process */</font>
<a name="line267">267: </a>      size_t n = 0;   <font color="#B22222">/* dummy value so MPI-Uni doesn't think it is not set*/</font>
<a name="line268">268: </a>      void   *ptr;
<a name="line269">269: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;n,1,MPIU_SIZE_T,0,comm);
<a name="line270">270: </a>      <font color="#B22222">/* cannot use <A href="../../../docs/manualpages/Sys/PetscNew.html#PetscNew">PetscNew</A>() cause it requires struct argument */</font>
<a name="line271">271: </a>      <A href="../../../docs/manualpages/Sys/PetscMalloc.html#PetscMalloc">PetscMalloc</A>(n,&amp;ptr);
<a name="line272">272: </a>      <A href="../../../docs/manualpages/Sys/PetscMemzero.html#PetscMemzero">PetscMemzero</A>(ptr,n);

<a name="line274">274: </a>      objects[numberobjects++] = ptr;
<a name="line275">275: </a>      <font color="#4169E1">break</font>;
<a name="line276">276: </a>    }
<a name="line277">277: </a>    <font color="#4169E1">case</font> 1: {  <font color="#B22222">/* free some memory on this worker process */</font>
<a name="line278">278: </a>      <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A> i;
<a name="line279">279: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;i,1,MPIU_INT,0,comm);
<a name="line280">280: </a>      <A href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</A>(objects[i]);
<a name="line281">281: </a>      <font color="#4169E1">break</font>;
<a name="line282">282: </a>    }
<a name="line283">283: </a>    <font color="#4169E1">case</font> 2: {  <font color="#B22222">/* run a function on this worker process */</font>
<a name="line284">284: </a>      <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       i;
<a name="line285">285: </a>      <A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A> (*f)(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>,void*);
<a name="line286">286: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;i,1,MPIU_INT,0,comm);
<a name="line287">287: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>((PETSC_UINTPTR_T*)&amp;f,1,MPIU_SIZE_T,0,comm);
<a name="line288">288: </a>      (*f)(comm,objects[i]);
<a name="line289">289: </a>      <font color="#4169E1">break</font>;
<a name="line290">290: </a>    }
<a name="line291">291: </a>    <font color="#4169E1">case</font> 4: {  <font color="#B22222">/* run a function on this worker process with provided context */</font>
<a name="line292">292: </a>      <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       i;
<a name="line293">293: </a>      <A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A> (*f)(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>,void*,void*);
<a name="line294">294: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;i,1,MPIU_INT,0,comm);
<a name="line295">295: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>((PETSC_UINTPTR_T*)&amp;f,1,MPIU_SIZE_T,0,comm);
<a name="line296">296: </a>      (*f)(comm,PetscHMPICtx,objects[i]);
<a name="line297">297: </a>      <font color="#4169E1">break</font>;
<a name="line298">298: </a>    }
<a name="line299">299: </a>    <font color="#4169E1">case</font> 3: {
<a name="line300">300: </a>      exitwhileloop = <A href="../../../docs/manualpages/Sys/PETSC_TRUE.html#PETSC_TRUE">PETSC_TRUE</A>;
<a name="line301">301: </a>      <font color="#4169E1">break</font>;
<a name="line302">302: </a>    }
<a name="line303">303: </a><strong><font color="#FF0000">    default:</font></strong>
<a name="line304">304: </a>      <A href="../../../docs/manualpages/Sys/SETERRQ1.html#SETERRQ1">SETERRQ1</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,PETSC_ERR_PLIB,<font color="#666666">"Unknown HMPI command %D"</font>,command);
<a name="line305">305: </a>    }
<a name="line306">306: </a>  }
<a name="line307">307: </a>  <font color="#4169E1">return</font>(0);
<a name="line308">308: </a>}

<a name="line312">312: </a><font color="#B22222">/*@C</font>
<a name="line313">313: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscHMPIMalloc.html#PetscHMPIMalloc">PetscHMPIMalloc</A> - Creates a "c struct" on all nodes of an HMPI communicator</font>

<a name="line315">315: </a><font color="#B22222">   Collective on <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A></font>

<a name="line317">317: </a><font color="#B22222">   Input Parameters:</font>
<a name="line318">318: </a><font color="#B22222">+   comm - Must be PETSC_COMM_LOCAL_WORLD</font>
<a name="line319">319: </a><font color="#B22222">-   n  - amount of memory requested</font>

<a name="line321">321: </a><font color="#B22222">   Level: developer</font>

<a name="line323">323: </a><font color="#B22222">   Developer Notes: Since comm must be PETSC_COMM_LOCAL_WORLD, why have this argument?</font>

<a name="line325">325: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A>(), PCHMPIRun(), PCHMPIFree()</font>

<a name="line327">327: </a><font color="#B22222">@*/</font>
<a name="line328">328: </a><strong><font color="#4169E1"><a name="PetscHMPIMalloc"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscHMPIMalloc.html#PetscHMPIMalloc">PetscHMPIMalloc</A>(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> comm,size_t n,void **ptr)</font></strong>
<a name="line329">329: </a>{
<a name="line331">331: </a>  <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       command = 0;

<a name="line334">334: </a>  <font color="#4169E1">if</font> (PetscHMPIWorker) <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,PETSC_ERR_ARG_WRONGSTATE,<font color="#666666">"Not using HMPI feature of PETSc"</font>);

<a name="line336">336: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;command,1,MPIU_INT,0,comm);
<a name="line337">337: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;n,1,MPIU_SIZE_T,0,comm);

<a name="line339">339: </a>  <font color="#B22222">/* cannot use <A href="../../../docs/manualpages/Sys/PetscNew.html#PetscNew">PetscNew</A>() cause it requires struct argument */</font>
<a name="line340">340: </a>  <A href="../../../docs/manualpages/Sys/PetscMalloc.html#PetscMalloc">PetscMalloc</A>(n,ptr);
<a name="line341">341: </a>  <A href="../../../docs/manualpages/Sys/PetscMemzero.html#PetscMemzero">PetscMemzero</A>(*ptr,n);

<a name="line343">343: </a>  objects[numberobjects++] = *ptr;
<a name="line344">344: </a>  <font color="#4169E1">return</font>(0);
<a name="line345">345: </a>}

<a name="line349">349: </a><font color="#B22222">/*@C</font>
<a name="line350">350: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscHMPIFree.html#PetscHMPIFree">PetscHMPIFree</A> - Frees a "c struct" on all nodes of an HMPI communicator</font>

<a name="line352">352: </a><font color="#B22222">   Collective on <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A></font>

<a name="line354">354: </a><font color="#B22222">   Input Parameters:</font>
<a name="line355">355: </a><font color="#B22222">+   comm - Must be PETSC_COMM_LOCAL_WORLD</font>
<a name="line356">356: </a><font color="#B22222">-   ptr - pointer to data to be freed, must have been obtained with <A href="../../../docs/manualpages/Sys/PetscHMPIMalloc.html#PetscHMPIMalloc">PetscHMPIMalloc</A>()</font>

<a name="line358">358: </a><font color="#B22222">   Level: developer</font>

<a name="line360">360: </a><font color="#B22222">  Developer Notes: Since comm must be PETSC_COMM_LOCAL_WORLD, why have this argument?</font>

<a name="line362">362: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIMalloc.html#PetscHMPIMalloc">PetscHMPIMalloc</A>()</font>

<a name="line364">364: </a><font color="#B22222">@*/</font>
<a name="line365">365: </a><strong><font color="#4169E1"><a name="PetscHMPIFree"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscHMPIFree.html#PetscHMPIFree">PetscHMPIFree</A>(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> comm,void *ptr)</font></strong>
<a name="line366">366: </a>{
<a name="line368">368: </a>  <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       command = 1,i;

<a name="line371">371: </a>  <font color="#4169E1">if</font> (PetscHMPIWorker) <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,PETSC_ERR_ARG_WRONGSTATE,<font color="#666666">"Not using HMPI feature of PETSc"</font>);

<a name="line373">373: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;command,1,MPIU_INT,0,comm);
<a name="line374">374: </a>  <font color="#4169E1">for</font> (i=0; i&lt;numberobjects; i++) {
<a name="line375">375: </a>    <font color="#4169E1">if</font> (objects[i] == ptr) {
<a name="line376">376: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;i,1,MPIU_INT,0,comm);
<a name="line377">377: </a>      <A href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</A>(objects[i]);
<a name="line378">378: </a>      <font color="#4169E1">return</font>(0);
<a name="line379">379: </a>    }
<a name="line380">380: </a>  }
<a name="line381">381: </a>  <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,PETSC_ERR_ARG_WRONG,<font color="#666666">"Pointer does not appear to have been created with <A href="../../../docs/manualpages/Sys/PetscHMPIMalloc.html#PetscHMPIMalloc">PetscHMPIMalloc</A>()"</font>);
<a name="line382">382: </a>  <A href="../../../docs/manualpages/Sys/PetscFunctionReturn.html#PetscFunctionReturn">PetscFunctionReturn</A>(ierr);
<a name="line383">383: </a>}

<a name="line387">387: </a><font color="#B22222">/*@C</font>
<a name="line388">388: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscHMPIRun.html#PetscHMPIRun">PetscHMPIRun</A> - runs a function on all the processes of a node</font>

<a name="line390">390: </a><font color="#B22222">   Collective on <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A></font>

<a name="line392">392: </a><font color="#B22222">   Input Parameters:</font>
<a name="line393">393: </a><font color="#B22222">+   comm - communicator to run function on, must be PETSC_COMM_LOCAL_WORLD</font>
<a name="line394">394: </a><font color="#B22222">.   f - function to run</font>
<a name="line395">395: </a><font color="#B22222">-   ptr - pointer to data to pass to function; must be obtained with <A href="../../../docs/manualpages/Sys/PetscHMPIMalloc.html#PetscHMPIMalloc">PetscHMPIMalloc</A>()</font>

<a name="line397">397: </a><font color="#B22222">   Level: developer</font>

<a name="line399">399: </a><font color="#B22222">   Developer Notes: Since comm must be PETSC_COMM_LOCAL_WORLD, why have this argument?</font>

<a name="line401">401: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIMalloc.html#PetscHMPIMalloc">PetscHMPIMalloc</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIFree.html#PetscHMPIFree">PetscHMPIFree</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIRunCtx.html#PetscHMPIRunCtx">PetscHMPIRunCtx</A>()</font>

<a name="line403">403: </a><font color="#B22222">@*/</font>
<a name="line404">404: </a><strong><font color="#4169E1"><a name="PetscHMPIRun"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscHMPIRun.html#PetscHMPIRun">PetscHMPIRun</A>(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> comm,<A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A> (*f)(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>,void*),void *ptr)</font></strong>
<a name="line405">405: </a>{
<a name="line407">407: </a>  <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       command = 2,i;

<a name="line410">410: </a>  <font color="#4169E1">if</font> (PetscHMPIWorker) <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,PETSC_ERR_ARG_WRONGSTATE,<font color="#666666">"Not using HMPI feature of PETSc"</font>);

<a name="line412">412: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;command,1,MPIU_INT,0,comm);
<a name="line413">413: </a>  <font color="#4169E1">for</font> (i=0; i&lt;numberobjects; i++) {
<a name="line414">414: </a>    <font color="#4169E1">if</font> (objects[i] == ptr) {
<a name="line415">415: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;i,1,MPIU_INT,0,comm);
<a name="line416">416: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>((PETSC_UINTPTR_T*)&amp;f,1,MPIU_SIZE_T,0,comm);
<a name="line417">417: </a>      (*f)(comm,ptr);
<a name="line418">418: </a>      <font color="#4169E1">return</font>(0);
<a name="line419">419: </a>    }
<a name="line420">420: </a>  }
<a name="line421">421: </a>  <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,PETSC_ERR_ARG_WRONG,<font color="#666666">"Pointer does not appear to have been created with <A href="../../../docs/manualpages/Sys/PetscHMPIMalloc.html#PetscHMPIMalloc">PetscHMPIMalloc</A>()"</font>);
<a name="line422">422: </a>  <A href="../../../docs/manualpages/Sys/PetscFunctionReturn.html#PetscFunctionReturn">PetscFunctionReturn</A>(ierr);
<a name="line423">423: </a>}

<a name="line427">427: </a><font color="#B22222">/*@C</font>
<a name="line428">428: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscHMPIRunCtx.html#PetscHMPIRunCtx">PetscHMPIRunCtx</A> - runs a function on all the processes of a node</font>

<a name="line430">430: </a><font color="#B22222">   Collective on <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A></font>

<a name="line432">432: </a><font color="#B22222">   Input Parameters:</font>
<a name="line433">433: </a><font color="#B22222">+   comm - communicator to run function on, must be PETSC_COMM_LOCAL_WORLD</font>
<a name="line434">434: </a><font color="#B22222">.   f - function to run</font>
<a name="line435">435: </a><font color="#B22222">-   ptr - pointer to data to pass to function; must be obtained with <A href="../../../docs/manualpages/Sys/PetscHMPIMalloc.html#PetscHMPIMalloc">PetscHMPIMalloc</A>()</font>

<a name="line437">437: </a><font color="#B22222">   Notes: This is like <A href="../../../docs/manualpages/Sys/PetscHMPIRun.html#PetscHMPIRun">PetscHMPIRun</A>() except it also passes the context passed in <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A>()</font>
<a name="line438">438: </a><font color="#B22222">   Level: developer</font>

<a name="line440">440: </a><font color="#B22222">   Developer Notes: Since comm must be PETSC_COMM_LOCAL_WORLD, why have this argument?</font>

<a name="line442">442: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscHMPIMerge.html#PetscHMPIMerge">PetscHMPIMerge</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIMalloc.html#PetscHMPIMalloc">PetscHMPIMalloc</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIFree.html#PetscHMPIFree">PetscHMPIFree</A>(), <A href="../../../docs/manualpages/Sys/PetscHMPIRun.html#PetscHMPIRun">PetscHMPIRun</A>()</font>

<a name="line444">444: </a><font color="#B22222">@*/</font>
<a name="line445">445: </a><strong><font color="#4169E1"><a name="PetscHMPIRunCtx"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscHMPIRunCtx.html#PetscHMPIRunCtx">PetscHMPIRunCtx</A>(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> comm,<A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A> (*f)(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>,void*,void*),void *ptr)</font></strong>
<a name="line446">446: </a>{
<a name="line448">448: </a>  <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       command = 4,i;

<a name="line451">451: </a>  <font color="#4169E1">if</font> (PetscHMPIWorker) <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,PETSC_ERR_ARG_WRONGSTATE,<font color="#666666">"Not using HMPI feature of PETSc"</font>);

<a name="line453">453: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;command,1,MPIU_INT,0,comm);
<a name="line454">454: </a>  <font color="#4169E1">for</font> (i=0; i&lt;numberobjects; i++) {
<a name="line455">455: </a>    <font color="#4169E1">if</font> (objects[i] == ptr) {
<a name="line456">456: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;i,1,MPIU_INT,0,comm);
<a name="line457">457: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>((PETSC_UINTPTR_T*)&amp;f,1,MPIU_SIZE_T,0,comm);
<a name="line458">458: </a>      (*f)(comm,PetscHMPICtx,ptr);
<a name="line459">459: </a>      <font color="#4169E1">return</font>(0);
<a name="line460">460: </a>    }
<a name="line461">461: </a>  }
<a name="line462">462: </a>  <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,PETSC_ERR_ARG_WRONG,<font color="#666666">"Pointer does not appear to have been created with <A href="../../../docs/manualpages/Sys/PetscHMPIMalloc.html#PetscHMPIMalloc">PetscHMPIMalloc</A>()"</font>);
<a name="line463">463: </a>  <A href="../../../docs/manualpages/Sys/PetscFunctionReturn.html#PetscFunctionReturn">PetscFunctionReturn</A>(ierr);
<a name="line464">464: </a>}
</pre>
</body>

</html>
